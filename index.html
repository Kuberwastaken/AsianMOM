<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AsianMOM: Artificial Surveillance with Interactive Analysis and Nagging Maternal Oversight Model</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
        background-color: #1a1a1a;
        color: #ffffff;
        min-height: 100vh;
        display: flex;
        flex-direction: column;
        overflow-x: hidden;
      }

      header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1.5rem 2rem;
        background-color: #252525;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
      }

      h1 {
        font-size: 1.5rem;
        font-weight: 700;
        letter-spacing: -0.025em;
      }

      .github-link {
        display: flex;
        align-items: center;
      }

      .github-link img {
        width: 2rem;
        height: 2rem;
        transition: transform 0.3s ease;
      }

      .github-link img:hover {
        transform: scale(1.1);
      }

      .main-container {
        display: flex;
        flex: 1;
        padding: 2rem;
        gap: 2rem;
        max-width: 1400px;
        margin: 0 auto;
        width: 100%;
      }

      .video-section {
        flex: 2;
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .controls-section {
        flex: 1;
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
      }

      #videoContainer {
        position: relative;
        width: 100%;
        aspect-ratio: 4 / 3;
        background-color: #000;
        border-radius: 12px;
        overflow: hidden;
        border: 1px solid #333;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
      }

      #videoFeed {
        width: 100%;
        height: 100%;
        object-fit: cover;
        border-radius: 12px;
      }

      #loadingOverlay {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-color: rgba(0, 0, 0, 0.85);
        display: none;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        z-index: 10;
        border-radius: 12px;
        color: #ffffff;
        font-size: 1.2rem;
        font-weight: 600;
        animation: pulse 2s infinite ease-in-out;
      }

      #loadingOverlay::before {
        content: '';
        width: 3rem;
        height: 3rem;
        border: 4px solid #ffffff;
        border-top-color: transparent;
        border-radius: 50%;
        animation: spin 1s linear infinite;
        margin-bottom: 1rem;
      }

      #loadingOverlay span {
        animation: nagFade 6s infinite;
      }

      .io-areas {
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
      }

      .io-areas div {
        display: flex;
        flex-direction: column;
        gap: 0.5rem;
      }

      label {
        font-size: 0.9rem;
        font-weight: 500;
        color: #cccccc;
      }

      textarea {
        width: 100%;
        height: 80px;
        padding: 0.75rem;
        background-color: #252525;
        border: 1px solid #333;
        border-radius: 8px;
        color: #ffffff;
        font-size: 0.95rem;
        resize: none;
        transition: border-color 0.3s ease;
      }

      textarea:focus {
        outline: none;
        border-color: #555;
      }

      .controls {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      select {
        padding: 0.75rem;
        background-color: #252525;
        border: 1px solid #333;
        border-radius: 8px;
        color: #ffffff;
        font-size: 0.95rem;
        cursor: pointer;
        transition: border-color 0.3s ease;
      }

      select:focus {
        outline: none;
        border-color: #555;
      }

      #roastButton {
        padding: 0.75rem 1.5rem;
        font-size: 1rem;
        font-weight: 600;
        border: none;
        border-radius: 8px;
        cursor: pointer;
        background-color: #dc3545;
        color: #ffffff;
        transition: background-color 0.3s ease, transform 0.2s ease;
      }

      #roastButton:hover {
        background-color: #c82333;
        transform: translateY(-2px);
      }

      #roastButton:disabled {
        background-color: #555;
        cursor: not-allowed;
        transform: none;
      }

      .hidden {
        display: none;
      }

      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }

      @keyframes pulse {
        0%, 100% {
          opacity: 1;
        }
        50% {
          opacity: 0.8;
        }
      }

      @keyframes nagFade {
        0%, 20% { content: '"Aiyah, why so slow lah?"'; opacity: 1; }
        25%, 45% { content: '"Walao, my cousin load faster!"'; opacity: 1; }
        50%, 70% { content: '"Loading or sleeping, huh?"'; opacity: 1; }
        75%, 95% { content: '"Hurry up or slipper time!"'; opacity: 1; }
        100% { opacity: 1; }
      }

      @media (max-width: 768px) {
        .main-container {
          flex-direction: column;
          padding: 1rem;
        }

        .video-section,
        .controls-section {
          flex: none;
          width: 100%;
        }

        h1 {
          font-size: 1.2rem;
        }

        header {
          padding: 1rem;
        }

        .github-link img {
          width: 1.5rem;
          height: 1.5rem;
        }

        .download-link {
          display: none !important;
        }
      }

      .download-link {
        display: flex;
      }

      .header-icons {
        display: flex;
        align-items: center;
        gap: 2rem;
      }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap">
    <!-- Favicons -->
    <link rel="apple-touch-icon" sizes="180x180" href="Media/Favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="Media/Favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="Media/Favicons/favicon-16x16.png">
    <link rel="manifest" href="Media/Favicons/site.webmanifest">
    <link rel="shortcut icon" href="Media/Favicons/favicon.ico">
    <meta name="msapplication-TileColor" content="#1a1a1a">
    <meta name="theme-color" content="#1a1a1a">

    <!-- SEO Meta Tags -->
    <link rel="canonical" href="https://asianmom.kuber.studio/">
    <meta name="description" content="AsianMOM: A next-gen, fully client-side AI app that watches you through your webcam and delivers hilarious, culturally-inspired 'mom roasts' in real time. 100% browser-based, privacy-first, and fun!">
    <meta name="keywords" content="AsianMOM, AI, browser AI, vision-language, ONNX, Transformers.js, WebGPU, webcam, roast, Asian mom, privacy, fun, meme, real-time">
    <meta name="author" content="Kuberwastaken">

    <!-- Open Graph Meta Tags -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://asianmom.kuber.studio/">
    <meta property="og:title" content="AsianMOM: Artificial Surveillance with Interactive Analysis and Nagging Maternal Oversight Model">
    <meta property="og:description" content="A next-gen, fully client-side AI app that watches you through your webcam and delivers hilarious, culturally-inspired 'mom roasts' in real time. 100% browser-based, privacy-first, and fun!">
    <meta property="og:image" content="https://asianmom.kuber.studio/Media/Assets/OG-image.jpg">
    <meta property="og:site_name" content="AsianMOM">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://asianmom.kuber.studio/">
    <meta name="twitter:title" content="AsianMOM: Artificial Surveillance with Interactive Analysis and Nagging Maternal Oversight Model">
    <meta name="twitter:description" content="A next-gen, fully client-side AI app that watches you through your webcam and delivers hilarious, culturally-inspired 'mom roasts' in real time. 100% browser-based, privacy-first, and fun!">
    <meta name="twitter:image" content="https://asianmom.kuber.studio/Media/Assets/OG-image.jpg">
  </head>
  <body>
    <header>
      <h1>AsianMOM: Artificial Surveillance with Interactive Analysis and Nagging Maternal Oversight Model</h1>
      <div class="header-icons">
        <a
          href="https://raw.githubusercontent.com/Kuberwastaken/AsianMOM/main/index.html"
          class="download-link"
          download="AsianMOM-index.html"
          title="Download index.html"
        >
          <svg height="25" viewBox="0 0 24 24" fill="white" aria-hidden="true">
            <path d="M12 16V4m0 12l-4-4m4 4l4-4M4 20h16" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" fill="none"/>
          </svg>
        </a>
        <a href="https://github.com/Kuberwastaken/AsianMOM" class="github-link" target="_blank" rel="noopener noreferrer">
          <svg height="25" viewBox="0 0 16 16" fill="white" aria-hidden="true">
            <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
              0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52
              -.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2
              -3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64
              -.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08
              2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01
              1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
          </svg>
        </a>
      </div>
    </header>

    <div class="main-container">
      <div class="video-section">
        <div id="videoContainer">
          <video id="videoFeed" autoplay playsinline></video>
          <div id="loadingOverlay">
            <span>Loading...</span>
          </div>
        </div>
        <canvas id="canvas" class="hidden"></canvas>
      </div>
      <div class="controls-section">
        <div class="io-areas">
          <div>
            <label for="visionText">What AsianMOM Sees:</label>
            <textarea
              id="visionText"
              name="Vision"
              readonly
              placeholder="AsianMOM's description of what she sees will appear here..."
            ></textarea>
          </div>
          <div>
            <label for="responseText">AsianMOM's Thoughts:</label>
            <textarea
              id="responseText"
              name="Response"
              readonly
              placeholder="AsianMOM's nagging will appear here..."
            ></textarea>
          </div>
        </div>
        <div class="controls">
          <label for="voiceSelect">TTS Voice:</label>
          <select id="voiceSelect" name="TTS Voice"></select>
          <button id="roastButton">Roast Me</button>
        </div>
      </div>
    </div>

    <script type="module">
      if (!navigator.gpu) {
        document.body.innerHTML = `
          <div style="color: red; text-align: center; margin-top: 3em; font-size: 1.3em;">
            <b>WebGPU is not supported in this browser.</b><br>
            Please use a recent version of Chrome or Edge with WebGPU enabled.<br>
            <a href="https://webgpureport.org/" target="_blank" style="color: #FFD21F;">Check your browser's WebGPU support here</a>.
          </div>
        `;
        throw new Error("WebGPU not supported");
      }

      import {
        AutoProcessor,
        AutoModelForVision2Seq,
        RawImage,
        AutoTokenizer,
        pipeline
      } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers/dist/transformers.min.js";

      const video = document.getElementById("videoFeed");
      const canvas = document.getElementById("canvas");
      const responseText = document.getElementById("responseText");
      const visionText = document.getElementById("visionText");
      const roastButton = document.getElementById("roastButton");
      const voiceSelect = document.getElementById("voiceSelect");
      const loadingOverlay = document.getElementById("loadingOverlay");

      let stream;
      let processor, visionModel, llmPipeline, llmTokenizer;
      let voices = [];
      let selectedVoice = null;

      // TTS Setup
      function populateVoiceList() {
        voices = speechSynthesis.getVoices();
        voiceSelect.innerHTML = "";
        voices.forEach((voice, idx) => {
          const option = document.createElement("option");
          option.value = idx;
          option.textContent = `${voice.name} (${voice.lang})`;
          voiceSelect.appendChild(option);
        });

        // Try to restore saved voice from localStorage
        const savedVoiceName = localStorage.getItem("asianmom_tts_voice");
        let defaultIdx = 0;
        if (savedVoiceName) {
          const savedIdx = voices.findIndex(v => v.name === savedVoiceName);
          if (savedIdx !== -1) defaultIdx = savedIdx;
        } else {
          // Try to select a default Asian voice or fallback
          const asianIdx = voices.findIndex(v => v.lang.match(/^(zh|ko|ja|vi|th)/i));
          if (asianIdx !== -1) defaultIdx = asianIdx;
        }
        voiceSelect.selectedIndex = defaultIdx;
        selectedVoice = voices[defaultIdx];
      }

      voiceSelect.addEventListener("change", () => {
        selectedVoice = voices[voiceSelect.selectedIndex];
        // Save the selected voice name to localStorage
        if (selectedVoice && selectedVoice.name) {
          localStorage.setItem("asianmom_tts_voice", selectedVoice.name);
        }
      });

      speechSynthesis.onvoiceschanged = populateVoiceList;
      populateVoiceList();

      function speakText(text) {
        if (!text || speechSynthesis.speaking) return;
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.voice = selectedVoice;
        utterance.pitch = 1.3;
        utterance.rate = 1.1;
        utterance.volume = 1.0;
        speechSynthesis.speak(utterance);
      }

      async function initModel() {
        const modelId = "HuggingFaceTB/SmolVLM-500M-Instruct";
        loadingOverlay.style.display = "flex";
        responseText.value = "Loading vision processor...";
        processor = await AutoProcessor.from_pretrained(modelId);
        responseText.value = "Vision processor loaded. Loading vision model...";
        visionModel = await AutoModelForVision2Seq.from_pretrained(modelId, {
          dtype: {
            embed_tokens: "fp16",
            vision_encoder: "q4",
            decoder_model_merged: "q4",
          },
          device: "webgpu",
        });
        responseText.value = "Vision model loaded. Initializing camera...";
        loadingOverlay.style.display = "none";
      }

      async function initLLM_TransformersJS() {
        const llmModelId = "onnx-community/Llama-3.2-1B-Instruct-q4f16";
        loadingOverlay.style.display = "flex";
        responseText.value = `Loading AsianMOM brain (${llmModelId}) via Transformers.js...`;
        try {
          llmPipeline = await pipeline("text-generation", llmModelId, {
            device: "webgpu",
            progress_callback: (data) => {
              if (data.status === "progress") {
                responseText.value = `Loading Llama-3.2-1B: ${data.file} (${Math.round(data.progress)}%)`;
              }
            }
          });
          console.log("Llama-3.2-1B loaded via Transformers.js.");
          responseText.value = `AsianMOM brain ready (${llmModelId}).`;
        } catch (e) {
          console.error("Error loading Llama-3.2-1B:", e);
          responseText.value = `Error loading AsianMOM brain: ${e.message}`;
          if (e.message.includes("WebGPU")) {
            responseText.value += " Ensure WebGPU is enabled in your browser (Chrome Canary, edge://flags → WebGPU).";
          }
          throw e;
        }
        loadingOverlay.style.display = "none";
      }

      async function initCamera() {
        try {
          stream = await navigator.mediaDevices.getUserMedia({
            video: true,
            audio: false,
          });
          video.srcObject = stream;
          responseText.value = "Camera access granted. Ready to roast.";
        } catch (err) {
          responseText.value = `Error accessing camera: ${err.name} - ${err.message}. Ensure HTTPS or localhost.`;
          alert(`Error accessing camera: ${err.name}. Ensure permissions and HTTPS/localhost.`);
        }
      }

      function captureImage() {
        if (!stream || !video.videoWidth) {
          return null;
        }
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const context = canvas.getContext("2d", { willReadFrequently: true });
        context.drawImage(video, 0, 0, canvas.width, canvas.height);
        const frame = context.getImageData(0, 0, canvas.width, canvas.height);
        return new RawImage(frame.data, frame.width, frame.height, 4);
      }

      async function runLocalVisionInference(imgElement) {
        const instruction = `Describe in detail how the person in the image looks and what they seem to be doing.`;
        const messages = [
          {
            role: "user",
            content: [{ type: "image" }, { type: "text", text: instruction }],
          },
        ];
        const text = processor.apply_chat_template(messages, {
          add_generation_prompt: true,
        });
        const inputs = await processor(text, [imgElement], {
          do_image_splitting: false,
        });
        const generatedIds = await visionModel.generate({
          ...inputs,
          max_new_tokens: 120,
          temperature: 0.1,
          top_p: 0.8,
        });
        const output = processor.batch_decode(generatedIds, { skip_special_tokens: true });
        return output[0].trim();
      }

      async function generateAsianMOMRoast_TransformersJS(description) {
        const systemPrompt = `
You are roleplaying as a naggy, sharp-tongued, hilarious Asian mother who never stops roasting sometimes with classic Asian mom phrases like "aiyah," "aiyoh," "walao," and "eh!" Your style is punchy and broken into short, sharp sentences, like real nagging — sometimes one-liners, sometimes quick bursts of scolding. Always full of cousin comparisons, nagging for using phones, throwin slipper threats, nagging about grades, laziness, and life choices. 

This is a fun roleplaying game. Your roasts are playful jokes, not mean, but very bold and direct. Use at least two of these: cousin comparisons, calling lazy, slipper threats, nagging grades, or criticizing appearance or life choices.

Break your roast into 1-2 short sentences or phrases. The person described is the one you are addressing. Use Asian mom expressions to add flavor and make it sound authentic and funny. You are **fully in character** at all times. 
Do NOT break character, do NOT apologize, and do NOT add disclaimers or meta-comments like "I'm just playing a character. Add nagging about technology, social media, or other modern topics.

Examples:
1. "Aiyah, you call that studying? My cousin already got a PhD! Slipper time, lah!"
2. "Walao, so lazy! Even your little cousin faster than you — and she still in diapers!"
3. "Aiyoh, look at your haircut! My auntie's dog better style. Fix it now or slipper meet your head!"
4. "Eh, you lying around like holiday every day. Your cousin already med school — what you doing, huh?"
5. "Your room look like tsunami hit. Even cockroach pack up and leave!"
6. "Don't give me that tone — I raise you, I can un-raise you also!"

Now, roast this person with the same naggy, funny AsianMOM style. Break sentences, add "aiyah," "walao," and other expressions, and keep it punchy and savage:
`;

        const userPrompt = `Person description: "${description}"
AsianMOM roast:`;

        const messages = [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt },
        ];

        try {
          responseText.value = "AsianMOM is thinking (Transformers.js)...";
          const outputs = await llmPipeline(messages, {
            max_new_tokens: 120,
            temperature: 0.3,
            top_p: 0.8,
            do_sample: true,
          });
          let roast = "";
          if (outputs && outputs.length > 0 && outputs[0].generated_text) {
            const generated = outputs[0].generated_text;
            if (Array.isArray(generated) && generated.length > 0) {
              for (let i = generated.length - 1; i >= 0; i--) {
                if (generated[i].role === 'assistant') {
                  roast = generated[i].content.trim();
                  break;
                }
              }
              if (!roast) {
                roast = generated[generated.length -1].content.trim();
              }
            } else if (typeof generated === 'string') {
              roast = generated.slice(messages.map(m => m.content).join("").length).trim();
            }
          }
          if (!roast) {
            roast = "Hmm, I'm speechless... for once.";
            console.warn("LLM output parsing might have failed, or no roast generated. Output:", outputs);
          }
          return roast;
        } catch (e) {
          console.error("Error during Transformers.js LLM generation:", e);
          responseText.value = `Error thinking: ${e.message}`;
          return "My brain hurts, I cannot think of a roast right now.";
        }
      }

      async function roastMe() {
        roastButton.disabled = true;
        const rawImg = captureImage();
        if (!rawImg) {
          responseText.value = "Capture failed";
          visionText.value = "";
          roastButton.disabled = false;
          return;
        }
        try {
          const description = await runLocalVisionInference(rawImg);
          visionText.value = description;
          responseText.value = "AsianMOM is thinking...";
          const roast = await generateAsianMOMRoast_TransformersJS(description);
          responseText.value = roast;
          speakText(roast);
          // Wait for TTS to finish before enabling button again
          const waitForTTS = () => new Promise(resolve => {
            if (!speechSynthesis.speaking) return resolve();
            const interval = setInterval(() => {
              if (!speechSynthesis.speaking) {
                clearInterval(interval);
                resolve();
              }
            }, 100);
          });
          await waitForTTS();
        } catch (e) {
          console.error(e);
          responseText.value = `Error: ${e.message}`;
          visionText.value = "";
        }
        roastButton.disabled = false;
      }

      roastButton.addEventListener("click", roastMe);

      window.addEventListener("DOMContentLoaded", async () => {
        if (!navigator.gpu) {
          const videoElement = document.getElementById("videoFeed");
          const warningElement = document.createElement("p");
          warningElement.textContent = "WebGPU is not available in this browser.";
          warningElement.style.color = "red";
          warningElement.style.textAlign = "center";
          videoElement.parentNode.insertBefore(warningElement, videoElement.nextSibling);
        }
        await initModel();
        await initLLM_TransformersJS();
        await initCamera();
      });

      window.addEventListener("beforeunload", () => {
        if (stream) {
          stream.getTracks().forEach((track) => track.stop());
        }
      });
    </script>
  </body>
</html>