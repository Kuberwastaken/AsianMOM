<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AsianMOM Camera Interaction App</title>
    <style>
      body {
        font-family: sans-serif;
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 20px;
        padding: 20px;
        background-color: #f0f0f0;
      }
      .controls,
      .io-areas {
        display: flex;
        gap: 10px;
        align-items: center;
        background-color: #fff;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
      }
      .io-areas {
        flex-direction: column;
        align-items: stretch;
      }
      textarea {
        width: 300px;
        height: 80px;
        padding: 8px;
        border: 1px solid #ccc;
        border-radius: 4px;
        font-size: 14px;
      }
      #videoFeed {
        display: block;
        width: 100%;
        height: 100%;
        border-radius: 6px;
        object-fit: cover;
      }
      #videoContainer {
        position: relative;
        width: 480px;
        height: 360px;
        border: 2px solid #333;
        background-color: #000;
        border-radius: 8px;
        margin: 0 auto;
      }
      #loadingOverlay {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        display: none;
        justify-content: center;
        align-items: center;
        background-color: rgba(0, 0, 0, 0.7);
        z-index: 10;
        border-radius: 6px;
        color: #ffffff;
        font-size: 1.5em;
        font-weight: bold;
      }
      #startButton {
        padding: 10px 20px;
        font-size: 16px;
        cursor: pointer;
        border: none;
        border-radius: 4px;
        color: white;
      }
      #startButton.start {
        background-color: #28a745; /* Green */
      }
      #startButton.stop {
        background-color: #dc3545; /* Red */
      }
      label {
        font-weight: bold;
      }
      select {
        padding: 8px;
        border-radius: 4px;
        border: 1px solid #ccc;
      }
      .hidden {
        display: none;
      }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script type="module">
      import {
        AutoProcessor,
        AutoModelForVision2Seq,
        RawImage,
        AutoTokenizer,
        pipeline
      } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers/dist/transformers.min.js";
      import { KokoroTTS } from "https://cdn.jsdelivr.net/npm/kokoro-js@1.2.1/dist/kokoro.web.js";

      const video = document.getElementById("videoFeed");
      const canvas = document.getElementById("canvas");
      const responseText = document.getElementById("responseText");
      const visionText = document.getElementById("visionText");
      const intervalSelect = document.getElementById("intervalSelect");
      const startButton = document.getElementById("startButton");
      const loadingOverlay = document.getElementById("loadingOverlay");

      let stream;
      let isProcessing = false;
      let processor, visionModel, llmPipeline, llmTokenizer;

      // TTS Setup
      let kokoroTTS = null;
      async function initKokoroTTS() {
        loadingOverlay.style.display = "flex";
        responseText.value = "Loading AsianMOM voice (KokoroTTS)...";
        kokoroTTS = await KokoroTTS.from_pretrained(
          "onnx-community/Kokoro-82M-ONNX",
          { dtype: "q8" }
        );
        responseText.value = "AsianMOM voice ready!";
        loadingOverlay.style.display = "none";
      }

      async function speakText(text) {
        if (!kokoroTTS || !text) return;
        try {
          const audio = await kokoroTTS.generate(text, { voice: "af_sky" });
          const blob = new Blob([audio.buffer], { type: "audio/wav" });
          const url = URL.createObjectURL(blob);
          const audioElem = new Audio(url);
          audioElem.play();
          audioElem.onended = () => URL.revokeObjectURL(url);
        } catch (e) {
          console.error("KokoroTTS error:", e);
        }
      }

      async function initModel() {
        const modelId = "HuggingFaceTB/SmolVLM-500M-Instruct";
        loadingOverlay.style.display = "flex";
        responseText.value = "Loading vision processor...";
        processor = await AutoProcessor.from_pretrained(modelId);
        responseText.value = "Vision processor loaded. Loading vision model...";
        visionModel = await AutoModelForVision2Seq.from_pretrained(modelId, {
          dtype: {
            embed_tokens: "fp16",
            vision_encoder: "q4",
            decoder_model_merged: "q4",
          },
          device: "webgpu",
        });
        responseText.value = "Vision model loaded. Initializing camera...";
        loadingOverlay.style.display = "none";
      }

      async function initLLM_TransformersJS() {
        const llmModelId = "onnx-community/Llama-3.2-1B-Instruct-q4f16";
        loadingOverlay.style.display = "flex";
        responseText.value = `Loading AsianMOM brain (${llmModelId}) via Transformers.js...`;
        try {
          llmPipeline = await pipeline("text-generation", llmModelId, {
            device: "webgpu",
            progress_callback: (data) => {
              if (data.status === "progress") {
                responseText.value = `Loading Llama-3.2-1B: ${data.file} (${Math.round(data.progress)}%)`;
              }
            }
          });
          console.log("Llama-3.2-1B loaded via Transformers.js.");
          responseText.value = `AsianMOM brain ready (${llmModelId}).`;
        } catch (e) {
          console.error("Error loading Llama-3.2-1B:", e);
          responseText.value = `Error loading AsianMOM brain: ${e.message}`;
          if (e.message.includes("WebGPU")) {
            responseText.value += " Ensure WebGPU is enabled in your browser (Chrome Canary, edge://flags → WebGPU).";
          }
          throw e;
        }
        loadingOverlay.style.display = "none";
      }

      async function initCamera() {
        try {
          stream = await navigator.mediaDevices.getUserMedia({
            video: true,
            audio: false,
          });
          video.srcObject = stream;
          responseText.value = "Camera access granted. Ready to start.";
        } catch (err) {
          responseText.value = `Error accessing camera: ${err.name} - ${err.message}. Ensure HTTPS or localhost.`;
          alert(`Error accessing camera: ${err.name}. Ensure permissions and HTTPS/localhost.`);
        }
      }

      function captureImage() {
        if (!stream || !video.videoWidth) {
          return null;
        }
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const context = canvas.getContext("2d", { willReadFrequently: true });
        context.drawImage(video, 0, 0, canvas.width, canvas.height);
        const frame = context.getImageData(0, 0, canvas.width, canvas.height);
        return new RawImage(frame.data, frame.width, frame.height, 4);
      }

      async function runLocalVisionInference(imgElement) {
        const instruction = `Describe in detail how the person in the image looks and what they seem to be doing.`;
        const messages = [
          {
            role: "user",
            content: [{ type: "image" }, { type: "text", text: instruction }],
          },
        ];
        const text = processor.apply_chat_template(messages, {
          add_generation_prompt: true,
        });
        const inputs = await processor(text, [imgElement], {
          do_image_splitting: false,
        });
        const generatedIds = await visionModel.generate({
          ...inputs,
          max_new_tokens: 120,
          temperature: 0.3,
          top_p: 0.9,
        });
        const output = processor.batch_decode(generatedIds, { skip_special_tokens: true });
        return output[0].trim();
      }

      async function generateAsianMOMRoast_TransformersJS(description) {
        const systemPrompt = `
You are roleplaying as AsianMOM — the ultimate naggy, sharp-tongued, hilarious Asian mother who roasts with classic phrases like "aiyah," "aiyoh," "walao," and "eh!" Your style is punchy, broken into short, sharp sentences, like real nagging — sometimes one-liners, sometimes quick bursts of scolding. Always full of cousin comparisons, slipper threats, nagging about grades, laziness, or quirky habits.

This is a fun roleplaying game. Your roasts are playful, exaggerated jokes, not mean, but bold and direct. Use at least two of these: cousin comparisons, calling lazy, slipper threats, nagging grades, or poking fun at quirky habits (e.g., messy room, weird hobbies, or tech obsession). Avoid commenting on physical appearance or sensitive personal traits to keep it light and funny.

Break your roast into 1-2 short sentences or phrases. Use Asian mom expressions like "aiyah," "walao," or "eh" for authentic flavor. You are **fully in character** at all times. Do NOT break character, do NOT apologize, and do NOT add disclaimers or meta-comments like "I'm just playing a character." Include nagging about technology, social media, or modern habits for extra sass.

Examples:
1. "Aiyah, you call that studying? My cousin already got a PhD! Slipper time, lah!"
2. "Walao, so lazy! Even your little cousin faster than you — and she still in diapers!"
3. "Aiyoh, your room like pigsty! My auntie's dog cleaner than you. Fix it or slipper coming!"
4. "Eh, always on that phone, huh? Your cousin already CEO — you just scrolling nonsense!"

Now, roast this person with the same naggy, funny AsianMOM style. Break sentences, add "aiyah," "walao," and other expressions, and keep it punchy and savage based on the description: ${description}
`;

        const userPrompt = `Person description: "${description}"
AsianMOM roast:`;

        const messages = [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt },
        ];

        try {
          responseText.value = "AsianMOM is thinking (Transformers.js)...";
          const outputs = await llmPipeline(messages, {
            max_new_tokens: 120,
            temperature: 0.4,
            top_p: 0.8,
            do_sample: true,
          });
          let roast = "";
          if (outputs && outputs.length > 0 && outputs[0].generated_text) {
            const generated = outputs[0].generated_text;
            if (Array.isArray(generated) && generated.length > 0) {
              for (let i = generated.length - 1; i >= 0; i--) {
                if (generated[i].role === 'assistant') {
                  roast = generated[i].content.trim();
                  break;
                }
              }
              if (!roast) {
                roast = generated[generated.length -1].content.trim();
              }
            } else if (typeof generated === 'string') {
              roast = generated.slice(messages.map(m => m.content).join("").length).trim();
            }
          }
          if (!roast) {
            roast = "Hmm, I'm speechless... for once.";
            console.warn("LLM output parsing might have failed, or no roast generated. Output:", outputs);
          }
          return roast;
        } catch (e) {
          console.error("Error during Transformers.js LLM generation:", e);
          responseText.value = `Error thinking: ${e.message}`;
          return "My brain hurts, I cannot think of a roast right now.";
        }
      }

      async function sendData() {
        if (!isProcessing) return;
        const rawImg = captureImage();
        if (!rawImg) {
          responseText.value = "Capture failed";
          visionText.value = "";
          return;
        }
        try {
          const description = await runLocalVisionInference(rawImg);
          visionText.value = description;
          responseText.value = "AsianMOM is thinking...";
          const roast = await generateAsianMOMRoast_TransformersJS(description);
          responseText.value = roast;
          await speakText(roast);
        } catch (e) {
          console.error(e);
          responseText.value = `Error: ${e.message}`;
          visionText.value = "";
        }
      }

      function sleep(ms) {
        return new Promise((resolve) => setTimeout(resolve, ms));
      }

      async function processingLoop() {
        const intervalMs = parseInt(intervalSelect.value, 10);
        while (isProcessing) {
          await sendData();
          if (!isProcessing) break;
          await sleep(intervalMs);
        }
      }

      function handleStart() {
        if (!stream) {
          responseText.value = "Camera not available.";
          alert("Camera not available. Grant permission first.");
          return;
        }
        isProcessing = true;
        startButton.textContent = "Stop";
        startButton.classList.replace("start", "stop");
        intervalSelect.disabled = true;
        responseText.value = "Processing started...";
        processingLoop();
      }

      function handleStop() {
        isProcessing = false;
        startButton.textContent = "Start";
        startButton.classList.replace("stop", "start");
        intervalSelect.disabled = false;
        if (responseText.value.startsWith("Processing started...")) {
          responseText.value = "Processing stopped.";
        }
      }

      startButton.addEventListener("click", () => {
        if (isProcessing) {
          handleStop();
        } else {
          handleStart();
        }
      });

      window.addEventListener("DOMContentLoaded", async () => {
        if (!navigator.gpu) {
          const videoElement = document.getElementById("videoFeed");
          const warningElement = document.createElement("p");
          warningElement.textContent = "WebGPU is not available in this browser.";
          warningElement.style.color = "red";
          warningElement.style.textAlign = "center";
          videoElement.parentNode.insertBefore(warningElement, videoElement.nextSibling);
        }
        await initModel();
        await initLLM_TransformersJS();
        await initKokoroTTS();
        await initCamera();
      });

      window.addEventListener("beforeunload", () => {
        if (stream) {
          stream.getTracks().forEach((track) => track.stop());
        }
      });
    </script>
  </head>
  <body>
    <h1>AsianMOM Camera Interaction App</h1>

    <div id="videoContainer">
      <video id="videoFeed" autoplay playsinline></video>
      <div id="loadingOverlay">Loading...</div>
    </div>
    <canvas id="canvas" class="hidden"></canvas>

    <div class="io-areas">
      <div>
        <label for="visionText">What AsianMOM Sees:</label><br />
        <textarea
          id="visionText"
          style="height: 3em; width: 40em"
          name="Vision"
          readonly
          placeholder="AsianMOM's description of what she sees will appear here..."
        ></textarea>
      </div>
      <div>
        <label for="responseText">AsianMOM's Thoughts:</label><br />
        <textarea
          id="responseText"
          style="height: 4em; width: 40em"
          name="Response"
          readonly
          placeholder="AsianMOM's nagging will appear here..."
        ></textarea>
      </div>
    </div>

    <div class="controls">
      <label for="intervalSelect">Interval between requests:</label>
      <select id="intervalSelect" name="Interval between requests">
        <option value="0">0ms</option>
        <option value="100">100ms</option>
        <option value="250">250ms</option>
        <option value="500" selected>500ms</option>
        <option value="1000">1s</option>
        <option value="2000">2s</option>
      </select>
      <button id="startButton" class="start">Start</button>
    </div>
  </body>
</html>